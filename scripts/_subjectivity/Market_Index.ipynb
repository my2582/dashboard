{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'progressbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99b12aac3019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0migraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'progressbar'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import progressbar\n",
    "import igraph as ig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(ret, thresh=0.95, window=250, enddate=\"2017-01-24\", shrinkage=None, index_ret=None, exp_shrinkage_theta=125,detrended=False):\n",
    "    \"\"\"Generates correlation matrix for a window that ends on enddate. Correlation can have exponential shrinkage (giving more weights to recent observations.)\n",
    "    index_ret is used for detrending. If None, will use average return of all assets.\n",
    "    Will only use assets with more than thresh%% data available in the window\"\"\"\n",
    "    end = list(ret.index).index(enddate) + 1\n",
    "    start = end - window\n",
    "    subret = ret.values[start:end]\n",
    "    if not (index_ret is None):\n",
    "        end = list(index_ret.index).index(enddate) + 1\n",
    "        start = end - window\n",
    "        index_subret = index_ret.values[start:end].flatten()\n",
    "    eligible = (~np.isnan(subret)).sum(axis=0) >= thresh * window\n",
    "    subret = subret[:, eligible]\n",
    "    index_names = ret.columns[eligible]\n",
    "    # drop whole column when there are less than or equal to\n",
    "    # thresh number of non-nan entries in the window\n",
    "    # sub = ret[start:end]\n",
    "    subret[np.isnan(subret)] = 0\n",
    "    if detrended:\n",
    "        r = subret\n",
    "        if not (index_ret is None):\n",
    "            I = index_subret\n",
    "        else:\n",
    "            I = subret.mean(axis=1)\n",
    "        n = len(I)\n",
    "        alpha = (r.sum(axis=0) * (I * I).sum() - I.sum() * r.T.dot(I)) / (n * (I * I).sum() - (I.sum()) ** 2)\n",
    "        beta = (n * r.T.dot(I) - I.sum() * r.sum(axis=0)) / (n * (I * I).sum() - (I.sum()) ** 2)\n",
    "        c = r - alpha - np.outer(I, beta)\n",
    "        # temp = pd.DataFrame(c)\n",
    "        # temp.index = company_names\n",
    "        # temp.columns = company_names\n",
    "        subret = c\n",
    "    if shrinkage is None:\n",
    "        corr_mat = pd.DataFrame(np.corrcoef(subret, rowvar=False))\n",
    "        corr_mat.columns = index_names\n",
    "        corr_mat.index = index_names\n",
    "    # elif shrinkage == \"LedoitWolf\":\n",
    "    #     cov = ledoit_wolf(subret, assume_centered=True)[0]\n",
    "    #     std = np.sqrt(np.diagonal(cov))\n",
    "    #     corr_mat = (cov / std[:, None]).T / std[:, None]\n",
    "    #     np.fill_diagonal(corr_mat, 1.0)\n",
    "    #     corr_mat = pd.DataFrame(data=corr_mat, index=subret.columns, columns=subret.columns)\n",
    "    elif shrinkage == \"Exponential\":\n",
    "        stocknames = index_names\n",
    "        weight_list = np.exp((np.arange(1, window + 1) - window) / exp_shrinkage_theta)\n",
    "        weight_list = weight_list / weight_list.sum()\n",
    "        cov = np.cov(subret, rowvar=False, aweights=weight_list)\n",
    "        cov_diag = np.sqrt(np.diag(cov))\n",
    "        corr = (cov / cov_diag).T / cov_diag\n",
    "        corr_mat = pd.DataFrame(corr)\n",
    "        corr_mat.columns = stocknames\n",
    "        corr_mat.index = stocknames\n",
    "    else:\n",
    "        print(\"'shrinkage' can only be None or 'Exponential'\")\n",
    "        return None\n",
    "    # corr_mat.apply(lambda x:1-x**2 if not math.isnan(x) else np.nan)\n",
    "    return corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_corr(ret, thresh=0.95, inclusion=pd.DataFrame(), window=250, shrinkage=None,exp_shrinkage_theta=125, detrended=False, store=None):\n",
    "    \"\"\"Computes correlations on all dates in the ret dataframe\"\"\"\n",
    "    print(\"Computing all correlations with window=%s, shrinkage=%s, theta=%s...\" % (window, shrinkage, exp_shrinkage_theta))\n",
    "    if store is None:\n",
    "        allcorr = {}\n",
    "    else:\n",
    "        allcorr = store\n",
    "    alldates = ret.index\n",
    "    alldates.sort_values()\n",
    "    bar = progressbar.ProgressBar(max_value=len(alldates[window:]))\n",
    "    for d in alldates[window:]:\n",
    "        if inclusion.empty:\n",
    "            allcorr[str(d.strftime(\"%Y-%m-%d\"))] = corr_matrix(ret, thresh, window, enddate=d, shrinkage=shrinkage, exp_shrinkage_theta=exp_shrinkage_theta,detrended=detrended)\n",
    "        else:\n",
    "            eligible_stocks = list(inclusion[(inclusion['from']<=d) & ((inclusion['thru'].isnull()) | (inclusion['thru']>=d))]['PERMNO'].unique())\n",
    "            allcorr[str(d.strftime(\"%Y-%m-%d\"))] = \\\n",
    "            corr_matrix(ret[eligible_stocks], thresh, window, enddate=d, shrinkage=shrinkage, exp_shrinkage_theta = exp_shrinkage_theta, detrended=detrended)\n",
    "        bar+=1\n",
    "    alldates = np.array(sorted([s[-10:] for s in allcorr.keys()]))\n",
    "    return allcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(corr, method='gower'):\n",
    "    \"\"\"Builds igraph graph from correlation matrix.\"\"\"\n",
    "    if method == \"gower\":\n",
    "        def distance(weight):\n",
    "            return (2 - 2 * weight) ** 0.5  # gower\n",
    "    elif method == \"power\":\n",
    "        def distance(weight):\n",
    "            return 1 - weight ** 2  # power\n",
    "    node_names = corr.columns.values\n",
    "    g = ig.Graph.Weighted_Adjacency(corr.values.tolist(), mode=\"UNDIRECTED\", attr=\"weight\", loops=False)\n",
    "    g.vs['name'] = node_names\n",
    "    g.es['weight+1'] = np.array(g.es['weight']) + 1.0\n",
    "    g.es['length'] = distance(np.array(g.es['weight']))\n",
    "    g.es['absweight'] = np.abs(np.array(g.es['weight']))\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MST(corrs, method=\"gower\"):\n",
    "    \"\"\"Returns a dictionary of Minimum Spanning Tree for each end date and their graphs in a separate dict\"\"\"\n",
    "    trees = {}\n",
    "    graphs = {}\n",
    "    print(\"Creating MSTs...\")\n",
    "    for d in corrs.keys():\n",
    "        G = build_graph(corrs[d], method)\n",
    "        graphs[d[-10:]] = G\n",
    "        T = G.spanning_tree(return_tree=True, weights='length')\n",
    "        trees[d[-10:]] = T\n",
    "    return trees, graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_clusters(trees, method='Newman', n_of_clusters=None):\n",
    "    \"\"\"input: trees: iGraph trees\n",
    "              method: 'Newman' or 'ClausetNewman'\n",
    "        Returns dicts of the clusters as lists and igraph.clustering objects\"\"\"\n",
    "    print(\"Computing clusters...\")\n",
    "    sorteddates = sorted(trees.keys())\n",
    "    usabletrees = trees\n",
    "    ig.arpack_options.maxiter = 500000\n",
    "    clusters = {}\n",
    "    IGclusters = {}\n",
    "    print(\"Computing clusterings using method=%s, n_of_clusters=%s\" % (method, n_of_clusters))\n",
    "    bar = progressbar.ProgressBar(max_value=len(sorteddates))\n",
    "    count = 0\n",
    "    if method == 'Newman':\n",
    "        for t in sorteddates:\n",
    "            if len(usabletrees[t].vs) == 0:\n",
    "                sys.exit(\"There are no stocks with sufficient data on %s! Exiting now.\" % t)\n",
    "            if n_of_clusters is None:\n",
    "                c = usabletrees[t].community_leading_eigenvector(weights=\"weight+1\")\n",
    "            else:\n",
    "                if len(usabletrees[t].vs) < n_of_clusters:\n",
    "                    print(\n",
    "                        \"On %s, there are only %s available entities but requiring %s clusters. Use %s clusters instead.\" % (\n",
    "                            t, str(len(usabletrees[t].vs)), str(n_of_clusters), str(len(usabletrees[t].vs))))\n",
    "                extra = 0\n",
    "                length = 0\n",
    "                while length != n_of_clusters:\n",
    "                    c = usabletrees[t].community_leading_eigenvector(weights=\"weight+1\", clusters=min(n_of_clusters,\n",
    "                                                                                                      len(usabletrees[\n",
    "                                                                                                              t].vs)) + extra)\n",
    "                    if len(c) == length:\n",
    "                        break\n",
    "                    length = len(c)\n",
    "                    extra = extra + 1\n",
    "            IGclusters[t] = c\n",
    "            clusters[t] = list(c)\n",
    "            for i in range(0, len(c)):\n",
    "                clusters[t][i] = [usabletrees[t].vs[\"name\"][j] for j in c[i]]\n",
    "            count = count + 1\n",
    "            bar.update(count)\n",
    "        return clusters, IGclusters\n",
    "    elif method == 'ClausetNewman':\n",
    "        for t in sorteddates:\n",
    "            if len(usabletrees[t].vs) == 0:\n",
    "                sys.exit(\"There are no stocks with sufficient data on %s! Exiting now.\" % t)\n",
    "            if n_of_clusters is None:\n",
    "                c = usabletrees[t].community_fastgreedy(weights=\"weight+1\").as_clustering()\n",
    "            else:\n",
    "                if len(usabletrees[t].vs) < n_of_clusters:\n",
    "                    print(\n",
    "                        \"On %s, there are only %s available entities but requiring %s clusters. Use %s clusters instead.\" % (\n",
    "                            t, str(len(usabletrees[t].vs)), str(n_of_clusters), str(len(usabletrees[t].vs))))\n",
    "                c = usabletrees[t].community_fastgreedy(weights=\"weight+1\").as_clustering(\n",
    "                    n=min(n_of_clusters, len(usabletrees[t].vs)))\n",
    "            clusters[t] = list(c)\n",
    "            IGclusters[t] = c\n",
    "            for i in range(0, len(c)):\n",
    "                clusters[t][i] = [usabletrees[t].vs[\"name\"][j] for j in c[i]]\n",
    "            count = count + 1\n",
    "            bar.update(count)\n",
    "        return clusters, IGclusters\n",
    "    elif method == 'infomap':\n",
    "        for t in sorteddates:\n",
    "            if len(usabletrees[t].vs) == 0:\n",
    "                sys.exit(\"There are no stocks with sufficient data on %s! Exiting now.\" % t)\n",
    "            if len(usabletrees[t].vs) < n_of_clusters:\n",
    "                print(\n",
    "                    \"On %s, there are only %s available entities but requiring %s clusters. Use %s clusters instead.\" % (\n",
    "                        t, str(len(usabletrees[t].vs)), str(n_of_clusters), str(len(usabletrees[t].vs))))\n",
    "            c = usabletrees[t].community_infomap(edge_weights=\"weight+1\")\n",
    "            clusters[t] = list(c)\n",
    "            IGclusters[t] = c\n",
    "            for i in range(0, len(c)):\n",
    "                clusters[t][i] = [usabletrees[t].vs[\"name\"][j] for j in c[i]]\n",
    "            count = count + 1\n",
    "            bar.update(count)\n",
    "        return clusters, IGclusters\n",
    "    else:\n",
    "        print(\"'method' can only be 'Newman' or 'ClausetNewman' or 'infomap'. Your input was '%s'\" % method)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/eugene/Google Drive/Columbia Affiliation/Machine Learning for Global Risk/Market Data/IndexData_new.xlsx', sheet_name = 'Valuefrom 2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "IndexData = data.set_index('Date', drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexData=IndexData[754:4761]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 250: 2014-10-02 --- 2018-10-04\n",
    "all_correlation = all_corr(IndexData, thresh=0.75, shrinkage='Exponential',detrended = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correlation['2014-10-02'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees, graphs = MST(all_correlation, \"gower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = []\n",
    "\n",
    "import datetime\n",
    "keys = sorted(trees.keys(), key=lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "for i in keys:\n",
    "    length_list.append(sum(trees[i].es['weight']))\n",
    "\n",
    "length = pd.DataFrame(data = length_list, index = keys, columns = ['Length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(keys[0::10], length_list[0::10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "\n",
    "k = keys[0::50]\n",
    "l = length_list[0::50]\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "yearsFmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(k, l)\n",
    "\n",
    "# format the ticks\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_major_formatter(yearsFmt)\n",
    "ax.xaxis.set_minor_locator(months)\n",
    "\n",
    "# round to nearest years...\n",
    "datemin = np.datetime64(k[0], 'Y')\n",
    "datemax = np.datetime64(k[-1], 'Y') + np.timedelta64(1, 'Y')\n",
    "ax.set_xlim(str(datemin), str(datemax))\n",
    "\n",
    "\n",
    "# format the coords message box\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax.format_ydata = l\n",
    "\n",
    "# rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, IGclusters = construct_clusters(trees, method='ClausetNewman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['2017-01-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_clusters_list = []\n",
    "\n",
    "for i in keys:\n",
    "    n_of_clusters_list.append(len(clusters[i]))\n",
    "\n",
    "n_of_clusters = pd.DataFrame(data = n_of_clusters_list, index = keys, columns = ['Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_clusters.to_csv('/home/sbl/Documents/number_of_clusters.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length.to_csv('/home/sbl/Documents/length_of_MST.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
